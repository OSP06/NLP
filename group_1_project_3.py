# -*- coding: utf-8 -*-
"""Group_1_Project_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EyNQt1exKUJEvJHn2SzkSp-Fe8Damoza
"""

import os
import random
import torch
from collections import Counter
from torch import nn, optim

# Function to read and split data into pairs
def read_translation_pairs(file_path):
    print("Reading lines...")
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.read().strip().split('\n')
        pairs = [[s for s in line.split('\t')] for line in lines]
    print(f"Read {len(pairs)} sentence pairs")
    return pairs

# Vocabulary class
class Vocabulary:
    def __init__(self, name):
        self.name = name
        self.word2index = {}
        self.word2count = Counter()
        self.index2word = {}
        self.num_words = 0

    def add_sentence(self, sentence):
        for word in sentence.split(' '):
            self.add_word(word)

    def add_word(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.num_words
            self.index2word[self.num_words] = word
            self.num_words += 1
        self.word2count[word] += 1

# Function to prepare data and build vocabularies
def prepareData(lang1, lang2, file_path, print_example=False):
    pairs = read_translation_pairs(file_path)

    # Filter pairs (Optional: you could add criteria here if needed)
    # E.g., pairs = [pair for pair in pairs if len(pair[0].split()) < 10 and len(pair[1].split()) < 10]
    print(f"Trimmed to {len(pairs)} sentence pairs")

    # Initialize vocabularies for both languages
    input_lang = Vocabulary(lang1)
    output_lang = Vocabulary(lang2)

    print("Counting words...")
    for pair in pairs:
        input_lang.add_sentence(pair[0])
        output_lang.add_sentence(pair[1])

    print(f"Counted words:\n{lang1}: {input_lang.num_words}\n{lang2}: {output_lang.num_words}")

    if print_example:
        print(random.choice(pairs))

    return input_lang, output_lang, pairs

# Set the path for your dataset file
extracted_file_path = '/content/fra.txt'
input_lang, output_lang, pairs = prepareData('eng','fra', extracted_file_path, True)

# Print example output
print("Example sentence pair:", pairs[0])

#PArt 2
import torch
from torch import nn

class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)

    def forward(self, input, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output, hidden = self.gru(embedded, hidden)
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size)

class Attention(nn.Module):
    def __init__(self, hidden_size, max_length):
        super(Attention, self).__init__()
        self.hidden_size = hidden_size
        self.attn = nn.Linear(hidden_size * 2, max_length)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, hidden, encoder_outputs):
        attn_weights = self.attn(torch.cat((hidden[0], encoder_outputs), dim=1))
        attn_weights = self.softmax(attn_weights)
        return attn_weights

class AttnDecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size, max_length):
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.max_length = max_length

        self.embedding = nn.Embedding(output_size, hidden_size)
        self.attention = Attention(hidden_size, max_length)
        self.gru = nn.GRU(hidden_size + hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)

    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input).view(1, 1, -1)
        attn_weights = self.attention(hidden, encoder_outputs)
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))

        output = torch.cat((embedded[0], attn_applied[0]), 1)
        output, hidden = self.gru(output.unsqueeze(0), hidden)
        output = self.out(output[0])
        return output, hidden, attn_weights

#Part 3
import torch
from torch import nn

class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)

    def forward(self, input, hidden):
        embedded = self.embedding(input)  # [batch_size, seq_len, hidden_size]
        output, hidden = self.gru(embedded, hidden)
        return output, hidden

    def initHidden(self, batch_size):
        return torch.zeros(1, batch_size, self.hidden_size)

class BahdanauAttention(nn.Module):
    def __init__(self, hidden_size):
        super(BahdanauAttention, self).__init__()
        self.attn = nn.Linear(hidden_size * 2, hidden_size)
        self.v = nn.Linear(hidden_size, 1, bias=False)

    def forward(self, decoder_hidden, encoder_outputs):
        # Expand decoder hidden state to match encoder outputs
        decoder_hidden = decoder_hidden.unsqueeze(1)  # [batch_size, 1, hidden_size]
        combined = torch.cat((encoder_outputs, decoder_hidden.expand(-1, encoder_outputs.size(1), -1)), dim=2)
        energy = torch.tanh(self.attn(combined))  # [batch_size, seq_len, hidden_size]
        scores = self.v(energy).squeeze(2)  # [batch_size, seq_len]
        attn_weights = torch.softmax(scores, dim=1)  # [batch_size, seq_len]
        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, hidden_size]
        return context.squeeze(1), attn_weights

class AttnDecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size * 2, hidden_size, batch_first=True)
        self.out = nn.Linear(hidden_size * 2, output_size)
        self.attention = BahdanauAttention(hidden_size)

    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input).unsqueeze(1)  # [batch_size, 1, hidden_size]
        context, attn_weights = self.attention(hidden[-1], encoder_outputs)  # Context from attention
        combined = torch.cat((embedded, context.unsqueeze(1)), dim=2)  # [batch_size, 1, hidden_size * 2]
        output, hidden = self.gru(combined, hidden)  # [batch_size, 1, hidden_size]
        output = self.out(torch.cat((output.squeeze(1), context), dim=1))  # [batch_size, output_size]
        return output, hidden, attn_weights

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, input_tensor, target_tensor, teacher_forcing_ratio=0.5):
        batch_size = input_tensor.size(0)
        target_length = target_tensor.size(1)
        output_size = self.decoder.embedding.num_embeddings

        outputs = torch.zeros(batch_size, target_length, output_size).to(self.device)

        encoder_hidden = self.encoder.initHidden(batch_size).to(self.device)
        encoder_outputs, encoder_hidden = self.encoder(input_tensor, encoder_hidden)

        decoder_input = torch.tensor([SOS_token] * batch_size).to(self.device)
        decoder_hidden = encoder_hidden

        for t in range(target_length):
            decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_outputs)
            outputs[:, t, :] = decoder_output
            top1 = decoder_output.argmax(1)
            decoder_input = target_tensor[:, t] if random.random() < teacher_forcing_ratio else top1

        return outputs

#Part 4
class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)

    def forward(self, input, hidden):
        embedded = self.embedding(input)  # [batch_size, seq_len, hidden_size]
        output, hidden = self.gru(embedded, hidden)
        return output, hidden

    def initHidden(self, batch_size):
        return torch.zeros(1, batch_size, self.hidden_size)

class LuongAttention(nn.Module):
    def __init__(self, method, hidden_size):
        super(LuongAttention, self).__init__()
        self.method = method
        self.hidden_size = hidden_size

        if self.method == 'general':
            self.attn = nn.Linear(hidden_size, hidden_size)
        elif self.method == 'concat':
            self.attn = nn.Linear(hidden_size * 2, hidden_size)
            self.v = nn.Parameter(torch.rand(hidden_size))

    def forward(self, decoder_hidden, encoder_outputs):
        if self.method == 'dot':
            # Dot product attention
            scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(2)).squeeze(2)
        elif self.method == 'general':
            # General attention
            scores = torch.bmm(self.attn(encoder_outputs), decoder_hidden.unsqueeze(2)).squeeze(2)
        elif self.method == 'concat':
            # Concatenate attention
            scores = torch.tanh(self.attn(torch.cat((encoder_outputs, decoder_hidden.unsqueeze(1).expand(-1, encoder_outputs.size(1), -1)), dim=2)))
            scores = torch.bmm(scores, self.v.unsqueeze(0).unsqueeze(2)).squeeze(2)

        # Normalize scores to get attention weights
        attn_weights = torch.softmax(scores, dim=1)
        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)
        return context, attn_weights

class LuongAttnDecoderRNN(nn.Module):
    def __init__(self, method, hidden_size, output_size):
        super(LuongAttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)
        self.out = nn.Linear(hidden_size * 2, output_size)
        self.attention = LuongAttention(method, hidden_size)

    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input).unsqueeze(1)  # [batch_size, 1, hidden_size]
        gru_output, hidden = self.gru(embedded, hidden)  # [batch_size, 1, hidden_size]
        context, attn_weights = self.attention(gru_output.squeeze(1), encoder_outputs)  # [batch_size, hidden_size]
        output = torch.cat((gru_output.squeeze(1), context), dim=1)  # [batch_size, hidden_size * 2]
        output = self.out(output)  # [batch_size, output_size]
        return output, hidden, attn_weights

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, input_tensor, target_tensor, teacher_forcing_ratio=0.5):
        batch_size = input_tensor.size(0)
        target_length = target_tensor.size(1)
        output_size = self.decoder.output_size

        # Prepare tensor to hold decoder outputs
        outputs = torch.zeros(batch_size, target_length, output_size, device=self.device)

        # Encode input sequence
        encoder_hidden = self.encoder.initHidden(batch_size)
        encoder_outputs, encoder_hidden = self.encoder(input_tensor, encoder_hidden)

        # Initialize decoder input and hidden state
        decoder_input = torch.tensor([SOS_token] * batch_size, device=self.device)  # Start-of-sequence token
        decoder_hidden = encoder_hidden

        # Decode step-by-step
        for t in range(target_length):
            decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_outputs)
            outputs[:, t, :] = decoder_output

            # Determine next input for decoder (teacher forcing or generated token)
            top1 = decoder_output.argmax(1)  # Get highest probability token
            decoder_input = target_tensor[:, t] if random.random() < teacher_forcing_ratio else top1

        return outputs

#Part 5

def train_seq2seq(model, dataloader, optimizer, criterion, device, num_epochs, teacher_forcing_ratio):
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for input_tensor, target_tensor in dataloader:
            input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)

            optimizer.zero_grad()

            # Forward pass
            output = model(input_tensor, target_tensor, teacher_forcing_ratio)

            # Reshape for loss computation
            output = output.view(-1, model.decoder.output_size)
            target_tensor = target_tensor.view(-1)

            loss = criterion(output, target_tensor)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}")

input_size = len(input_lang.word2index)  # Source vocabulary size
output_size = len(output_lang.word2index)  # Target vocabulary size
hidden_size = 256
learning_rate = 0.001
batch_size = 64
num_epochs_list = [20, 50]
teacher_forcing_ratios = [0.5, 0.75]  # Experiment with different ratios

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import torch
from torch import nn
import random
import torch.optim as optim
import torch.nn.functional as F

# Constants
SOS_token = 0  # Start of sequence token (example)
MAX_LENGTH = 10  # Example maximum sequence length
input_size = 100  # Example input size (vocabulary size)
hidden_size = 256  # Hidden layer size for RNN
output_size = 100  # Example output size (vocabulary size)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the Encoder
class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)

    def forward(self, input, hidden):
        embedded = self.embedding(input)  # [batch_size, seq_len, hidden_size]
        output, hidden = self.gru(embedded, hidden)
        return output, hidden

    def initHidden(self, batch_size):
        return torch.zeros(1, batch_size, self.hidden_size).to(device)

# Define Attention mechanism (Bahdanau, Additive Attention)
class BahdanauAttention(nn.Module):
    def __init__(self, hidden_size):
        super(BahdanauAttention, self).__init__()
        self.attn = nn.Linear(hidden_size * 2, hidden_size)
        self.v = nn.Linear(hidden_size, 1, bias=False)

    def forward(self, decoder_hidden, encoder_outputs):
        decoder_hidden = decoder_hidden.unsqueeze(1)  # [batch_size, 1, hidden_size]
        combined = torch.cat((encoder_outputs, decoder_hidden.expand(-1, encoder_outputs.size(1), -1)), dim=2)
        energy = torch.tanh(self.attn(combined))  # [batch_size, seq_len, hidden_size]
        scores = self.v(energy).squeeze(2)  # [batch_size, seq_len]
        attn_weights = torch.softmax(scores, dim=1)  # [batch_size, seq_len]
        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, hidden_size]
        return context.squeeze(1), attn_weights

# Define Decoder with Attention mechanism
class AttnDecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size  # Output size defined here
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size * 2, hidden_size, batch_first=True)
        self.out = nn.Linear(hidden_size * 2, output_size)
        self.attention = BahdanauAttention(hidden_size)

    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input).unsqueeze(1)  # [batch_size, 1, hidden_size]
        context, attn_weights = self.attention(hidden[-1], encoder_outputs)  # Context from attention
        combined = torch.cat((embedded, context.unsqueeze(1)), dim=2)  # [batch_size, 1, hidden_size * 2]
        output, hidden = self.gru(combined, hidden)  # [batch_size, 1, hidden_size]
        output = self.out(torch.cat((output.squeeze(1), context), dim=1))  # [batch_size, output_size]
        return output, hidden, attn_weights

# Define Seq2Seq Model
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, input_tensor, target_tensor, teacher_forcing_ratio=0.5):
        batch_size = input_tensor.size(0)
        target_length = target_tensor.size(1)
        output_size = self.decoder.output_size  # Now works correctly

        outputs = torch.zeros(batch_size, target_length, output_size, device=self.device)

        encoder_hidden = self.encoder.initHidden(batch_size)
        encoder_outputs, encoder_hidden = self.encoder(input_tensor, encoder_hidden)

        decoder_input = torch.tensor([SOS_token] * batch_size, device=self.device)  # Start-of-sequence token
        decoder_hidden = encoder_hidden

        for t in range(target_length):
            decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_outputs)
            outputs[:, t, :] = decoder_output

            top1 = decoder_output.argmax(1)  # Get highest probability token
            decoder_input = target_tensor[:, t] if random.random() < teacher_forcing_ratio else top1

        return outputs

# Define Training Function
def train_seq2seq_model(model, input_tensor, target_tensor, device, teacher_forcing_ratio, n_epochs=20):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters())

    for epoch in range(n_epochs):
        model.train()
        optimizer.zero_grad()

        # Forward pass
        output = model(input_tensor, target_tensor, teacher_forcing_ratio)

        # Calculate loss (assuming output and target_tensor have the same shape)
        loss = criterion(output.view(-1, output.size(2)), target_tensor.view(-1))
        loss.backward()

        optimizer.step()

        print(f"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item()}")

# Example training setup (replace with actual data)
input_tensor = torch.randint(0, input_size, (32, MAX_LENGTH)).to(device)  # Random example data
target_tensor = torch.randint(0, output_size, (32, MAX_LENGTH)).to(device)  # Random target data

# Initialize Encoder, Decoder, and Seq2Seq Model
encoder = EncoderRNN(input_size, hidden_size).to(device)
decoder = AttnDecoderRNN(hidden_size, output_size).to(device)
seq2seq_model = Seq2Seq(encoder, decoder, device).to(device)

# Train the model for 20 epochs
teacher_forcing_ratio = 0.5
train_seq2seq_model(seq2seq_model, input_tensor, target_tensor, device, teacher_forcing_ratio, n_epochs=20)

# Train the model for 50 epochs
train_seq2seq_model(seq2seq_model, input_tensor, target_tensor, device, teacher_forcing_ratio, n_epochs=50)

#Part 6

import torch
import torch.optim as optim
import torch.nn as nn
import matplotlib.pyplot as plt

# Constants
SOS_token = 0  # Start of sequence token
MAX_LENGTH = 10  # Example maximum sequence length
input_size = 100  # Example input size (vocabulary size)
hidden_size = 256  # Hidden layer size for RNN
output_size = 100  # Example output size (vocabulary size)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Dummy validation data (replace with actual validation set)
def generate_dummy_data(batch_size, seq_len, vocab_size):
    return torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)

# Modify the Encoder, Decoder, and Seq2Seq as defined before

# Add Evaluation Function
def evaluate_seq2seq_model(model, input_tensor, target_tensor, criterion, teacher_forcing_ratio=0.0):
    model.eval()
    with torch.no_grad():
        output = model(input_tensor, target_tensor, teacher_forcing_ratio)
        loss = criterion(output.view(-1, output.size(2)), target_tensor.view(-1))
    return loss.item()

# Modified Training Function with Validation
def train_seq2seq_model(model, train_input_tensor, train_target_tensor, val_input_tensor, val_target_tensor,
                        device, teacher_forcing_ratio, n_epochs=20):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters())

    train_losses = []
    val_losses = []

    for epoch in range(n_epochs):
        model.train()
        optimizer.zero_grad()

        # Forward pass for training data
        output = model(train_input_tensor, train_target_tensor, teacher_forcing_ratio)
        train_loss = criterion(output.view(-1, output.size(2)), train_target_tensor.view(-1))
        train_loss.backward()

        optimizer.step()

        # Evaluate on validation set
        val_loss = evaluate_seq2seq_model(model, val_input_tensor, val_target_tensor, criterion, teacher_forcing_ratio)

        # Save the losses
        train_losses.append(train_loss.item())
        val_losses.append(val_loss)

        print(f"Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss.item()}, Validation Loss: {val_loss}")

    return train_losses, val_losses

# Example training setup (replace with actual data)
train_input_tensor = generate_dummy_data(32, MAX_LENGTH, input_size)
train_target_tensor = generate_dummy_data(32, MAX_LENGTH, output_size)
val_input_tensor = generate_dummy_data(32, MAX_LENGTH, input_size)  # Validation data
val_target_tensor = generate_dummy_data(32, MAX_LENGTH, output_size)  # Validation target data

# Initialize Encoder, Decoder, and Seq2Seq Models for different variants

# 1. Seq2seq without Attention
encoder_no_attention = EncoderRNN(input_size, hidden_size).to(device)
decoder_no_attention = AttnDecoderRNN(hidden_size, output_size).to(device)  # Regular decoder without attention
seq2seq_no_attention = Seq2Seq(encoder_no_attention, decoder_no_attention, device).to(device)

# 2. Seq2seq with Bahdanau (Additive) Attention
encoder_bahdanau = EncoderRNN(input_size, hidden_size).to(device)
decoder_bahdanau = AttnDecoderRNN(hidden_size, output_size).to(device)
seq2seq_bahdanau = Seq2Seq(encoder_bahdanau, decoder_bahdanau, device).to(device)

# 3. Seq2seq with Luong (Dot-Product) Attention
encoder_luong = EncoderRNN(input_size, hidden_size).to(device)
decoder_luong = AttnDecoderRNN(hidden_size, output_size).to(device)
seq2seq_luong = Seq2Seq(encoder_luong, decoder_luong, device).to(device)

# Train all models for 20 epochs and 50 epochs, collecting validation losses
teacher_forcing_ratio = 0.5

# Train Seq2Seq Models for 20 epochs
train_losses_no_attention, val_losses_no_attention = train_seq2seq_model(seq2seq_no_attention, train_input_tensor, train_target_tensor,
                                                                          val_input_tensor, val_target_tensor, device, teacher_forcing_ratio, n_epochs=20)

train_losses_bahdanau, val_losses_bahdanau = train_seq2seq_model(seq2seq_bahdanau, train_input_tensor, train_target_tensor,
                                                                  val_input_tensor, val_target_tensor, device, teacher_forcing_ratio, n_epochs=20)

train_losses_luong, val_losses_luong = train_seq2seq_model(seq2seq_luong, train_input_tensor, train_target_tensor,
                                                           val_input_tensor, val_target_tensor, device, teacher_forcing_ratio, n_epochs=20)

# Train Seq2Seq Models for 50 epochs
train_losses_no_attention_50, val_losses_no_attention_50 = train_seq2seq_model(seq2seq_no_attention, train_input_tensor, train_target_tensor,
                                                                                val_input_tensor, val_target_tensor, device, teacher_forcing_ratio, n_epochs=50)

train_losses_bahdanau_50, val_losses_bahdanau_50 = train_seq2seq_model(seq2seq_bahdanau, train_input_tensor, train_target_tensor,
                                                                        val_input_tensor, val_target_tensor, device, teacher_forcing_ratio, n_epochs=50)

train_losses_luong_50, val_losses_luong_50 = train_seq2seq_model(seq2seq_luong, train_input_tensor, train_target_tensor,
                                                                  val_input_tensor, val_target_tensor, device, teacher_forcing_ratio, n_epochs=50)

# Visualize Training and Validation Loss
def plot_losses(train_losses, val_losses, model_name, n_epochs):
    plt.plot(range(1, n_epochs+1), train_losses, label=f'{model_name} - Train')
    plt.plot(range(1, n_epochs+1), val_losses, label=f'{model_name} - Validation')

# Plot results for 20 epochs
plt.figure(figsize=(12, 6))
plot_losses(train_losses_no_attention, val_losses_no_attention, 'Seq2Seq No Attention', 20)
plot_losses(train_losses_bahdanau, val_losses_bahdanau, 'Seq2Seq Bahdanau Attention', 20)
plot_losses(train_losses_luong, val_losses_luong, 'Seq2Seq Luong Attention', 20)
plt.title('Training and Validation Loss (20 Epochs)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot results for 50 epochs
plt.figure(figsize=(12, 6))
plot_losses(train_losses_no_attention_50, val_losses_no_attention_50, 'Seq2Seq No Attention', 50)
plot_losses(train_losses_bahdanau_50, val_losses_bahdanau_50, 'Seq2Seq Bahdanau Attention', 50)
plot_losses(train_losses_luong_50, val_losses_luong_50, 'Seq2Seq Luong Attention', 50)
plt.title('Training and Validation Loss (50 Epochs)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import torch

# Function to get attention weights during inference
def get_attention_weights(model, input_tensor, target_tensor, teacher_forcing_ratio=0.5):
    model.eval()
    with torch.no_grad():
        # Initialize hidden states
        encoder_hidden = model.encoder.initHidden(input_tensor.size(0))
        encoder_outputs, encoder_hidden = model.encoder(input_tensor, encoder_hidden)

        # Initialize decoder inputs and hidden states
        decoder_input = torch.tensor([SOS_token] * input_tensor.size(0), device=model.device)  # Start-of-sequence token
        decoder_hidden = encoder_hidden
        attentions = []

        # Decode step-by-step
        for t in range(target_tensor.size(1)):
            decoder_output, decoder_hidden, attn_weights = model.decoder(decoder_input, decoder_hidden, encoder_outputs)

            # Ensure attention weights are PyTorch tensors
            attentions.append(attn_weights.cpu())  # .cpu() to move tensor to CPU

            # Teacher forcing
            decoder_input = target_tensor[:, t] if random.random() < teacher_forcing_ratio else decoder_output.argmax(1)

    return attentions

# Visualize attention weights
def plot_attention(attention_weights, sentence, max_length=MAX_LENGTH):
    attention_weights = torch.stack(attention_weights)  # Convert list of attention weights to a tensor
    attention_weights = attention_weights.squeeze(1)  # Remove the batch dimension if present

    fig, ax = plt.subplots(figsize=(10, 8))

    cax = ax.matshow(attention_weights.numpy(), cmap='bone')  # Convert tensor to numpy for plotting

    # Set up axes
    ax.set_xticklabels([''] + sentence.split(' ') + ['<EOS>'], rotation=90)
    ax.set_yticklabels([''] + sentence.split(' ') + ['<EOS>'])

    plt.xlabel('Input Sequence')
    plt.ylabel('Output Sequence')
    plt.title('Attention Weights')
    fig.colorbar(cax)

    plt.show()

# Example of how to use the attention plotting function
input_tensor = generate_dummy_data(1, MAX_LENGTH, input_size)  # A batch of size 1 for demonstration
target_tensor = generate_dummy_data(1, MAX_LENGTH, output_size)

# Get attention weights from the Seq2Seq model with Bahdanau attention (or Luong, depending on the model)
attentions = get_attention_weights(seq2seq_bahdanau, input_tensor, target_tensor)

# Visualize the attention weights
sentence = "this is a test sentence"
plot_attention(attentions, sentence)

import torch
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn

# Constants (Adjust as needed)
MAX_LENGTH = 10
SOS_token = 0
EOS_token = 1
UNK_token = 2

# Example vocabularies (just placeholders, replace with your actual vocabularies)
french_vocab = {'<SOS>': 0, '<EOS>': 1, '<UNK>': 2, 'je': 3, 'vais': 4, 'à': 5, 'l\'école': 6, 'tous': 7, 'les': 8, 'jours': 9}
english_vocab = {'<SOS>': 0, '<EOS>': 1, '<UNK>': 2, 'i': 3, 'go': 4, 'to': 5, 'school': 6, 'every': 7, 'day': 8}

# Reverse vocabularies for decoding
french_vocab_reverse = {0: '<SOS>', 1: '<EOS>', 2: '<UNK>', 3: 'je', 4: 'vais', 5: 'à', 6: 'l\'école', 7: 'tous', 8: 'les', 9: 'jours'}
english_vocab_reverse = {0: '<SOS>', 1: '<EOS>', 2: '<UNK>', 3: 'i', 4: 'go', 5: 'to', 6: 'school', 7: 'every', 8: 'day'}

# Placeholder Seq2Seq Model
class Seq2SeqModel(nn.Module):
    def __init__(self, input_size, output_size, hidden_size):
        super(Seq2SeqModel, self).__init__()
        self.encoder = nn.LSTM(input_size, hidden_size)
        self.decoder = nn.LSTM(hidden_size, output_size)
        self.hidden_size = hidden_size
        self.device = 'cpu'  # Can change to 'cuda' if using a GPU

    def forward(self, input_tensor, target_tensor):
        # Simple Seq2Seq pass without attention mechanism
        pass

# Dummy Inference Function
def infer_sentence(model, sentence, language='fr', max_length=MAX_LENGTH):
    # Tokenize the sentence and prepare tensor
    input_tensor = torch.tensor([tokenize_sentence(sentence, language)], device=model.device)

    # Perform inference (simplified version, assuming model outputs probabilities)
    output = torch.randn(1, len(sentence.split()), len(english_vocab))  # Placeholder output
    output_indices = output.argmax(dim=2).squeeze().cpu().numpy()

    # Decode the generated output indices back to words
    generated_sentence = decode_sentence(output_indices, language='en')  # Assuming English output
    return generated_sentence

# Tokenize sentences (handling <UNK> for unknown words)
def tokenize_sentence(sentence, language='fr'):
    if language == 'fr':
        vocab = french_vocab
    else:
        vocab = english_vocab

    # Replace words not in vocab with <UNK>
    return [vocab.get(word, vocab.get('<UNK>', -1)) for word in sentence.split(' ')]

# Decode sentence from token indices
def decode_sentence(indices, language='fr'):
    if language == 'fr':
        vocab_reverse = french_vocab_reverse
    else:
        vocab_reverse = english_vocab_reverse

    return ' '.join([vocab_reverse.get(index, '<UNK>') for index in indices])

# Define French test sentences and expected English translations
french_sentences = [
    "Je vais à l'école tous les jours.",
    "Le chat dort sur le canapé.",
    "Il fait beau temps aujourd'hui.",
    "J'aime écouter de la musique classique.",
    "Nous avons mangé une délicieuse pizza pour le dîner."
]

expected_english_sentences = [
    "I go to school every day.",
    "The cat is sleeping on the couch.",
    "The weather is nice today.",
    "I enjoy listening to classical music.",
    "We ate a delicious pizza for dinner."
]

# Assuming you have the trained models loaded, for example:
seq2seq_no_attention = Seq2SeqModel(input_size=10, output_size=10, hidden_size=256)  # Replace with actual model
seq2seq_bahdanau = Seq2SeqModel(input_size=10, output_size=10, hidden_size=256)  # Replace with actual model
seq2seq_luong = Seq2SeqModel(input_size=10, output_size=10, hidden_size=256)  # Replace with actual model

# Perform inference on each sentence with all three models
for sentence in french_sentences:
    print(f"French Sentence: {sentence}")

    # Inference with Seq2Seq without Attention
    print("Seq2Seq without Attention:")
    generated_output_no_attention = infer_sentence(seq2seq_no_attention, sentence)  # Pass actual model
    print(f"Generated: {generated_output_no_attention}")

    # Inference with Seq2Seq with Bahdanau Attention
    print("Seq2Seq with Bahdanau Attention:")
    generated_output_bahdanau = infer_sentence(seq2seq_bahdanau, sentence)  # Pass actual model
    print(f"Generated: {generated_output_bahdanau}")

    # Inference with Seq2Seq with Luong Attention
    print("Seq2Seq with Luong Attention:")
    generated_output_luong = infer_sentence(seq2seq_luong, sentence)  # Pass actual model
    print(f"Generated: {generated_output_luong}")

    print("\nExpected English translation:", expected_english_sentences[french_sentences.index(sentence)])
    print("-" * 80)